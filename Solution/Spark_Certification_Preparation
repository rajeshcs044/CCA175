/home/rrajendran/data/retail_db/customers-tab-delimited

1)
val data = spark.read
.option("inferSchema", true)
.option("delimiter", "\t")
.csv("testData/retail_db/customers-tab-delimited")
.toDF("customer_id", "customer_fname", "customer_lname", "customer_email", "customer_password", "customer_street", "customer_city", "customer_state", "customer_zipcode") 

val result = data.filter($"customer_state" === "CA")
.select(concat_ws(" ", $"customer_fname", $"customer_lname").alias("customer_name"))

result.write
.text("testData/output_retail_db") 

//second approach
data.createOrReplaceTempView("customer_view")
val result = spark.sql("""select concat(customer_fname, ' ', customer_lname) as customer_name from customer_view where customer_state = 'CA'""")

2)
val data = spark.read
.parquet("testData/retail_db/orders_parquet")

val result = data.filter($"order_status" === "COMPLETE")
.select($"order_id", to_date(from_unixtime($"order_date" / 1000)).alias("order_date"), $"order_status")

result.write.
option("compression", "gzip")
.json("testData/output_retail_db/order_parquet")

//second approach
data.createOrReplaceTempView("order_view")
val result = spark.sql("""select order_id, to_date(from_unixtime(order_date / 1000)) as order_date, order_status from order_view""")

3)
val data = spark.read
.option("inferschema", "true")
.option("delimiter", "\t")
.csv("testData/retail_db/customers-tab-delimited")
.toDF("customer_id", "customer_fname", "customer_lname", "customer_email", "customer_password", "customer_street", "customer_city", "customer_state", "customer_zipcode") 

val result = data.filter($"customer_city" === "Caguas")

result.write
.option("compression", "snappy").
orc("testData/output_retail_db/customer_orc")

//second approach
data.createOrReplaceTempView("customer_view")
val result = spark.sql("""select * from customer_view where customer_city = 'Caguas'""")

4)
val data = spark.read
.option("inferSchema", "true")
.csv("testData/retail_db/categories")
.toDF("category_id", "category_department_id", "category_name")

val result = data.map(x => x.mkString("\t"))
result.write.text("testData/output_retail_db/categories_test")

5)
spark-shell --packages org.apache.spark:spark-avro_2.11:2.4.4
spark-shell --packages com.databricks:spark-avro_2.11:4.0.0

val data = spark.read
.format("com.databricks.spark.avro")
.load("testData/retail_db/products_avro")

val result = data.filter($"product_price" > 1000.0)

result.write
.option("compression", "snappy")
.parquet("testData/output_retail_db/products_parquet")

6)
val data = spark.read
.csv("testData/retail_db/products")
.toDF("product_id", "product_category_id", "product_name", "product_description", "product_price", "product_image")

val result = data.filter($"product_price" < 1000.0).filter($"product_name".like("%Vector Series%"))

result.write
.option("compression", "deflate")
.format("com.databricks.spark.avro")
.save("testData/output_retail_db/products_avro_3")

1)
val data = spark.read
.option("inferSchema", "true")
.csv("testData/retail_db/categories")
.toDF("category_id", "dept_id", "category_name")

val result = data.select($"category_id", $"category_name")

spark.sql("""
create table if not exists default.categories_parquet (
category_id INT,
category_name STRING
)
stored as parquet
""")

result.write
.format("com.databricks.spark.avro")
.mode("overwrite")
.saveAsTable("categories_parquet")

2)
val data = spark.read
.option("inferSchema", "true")
.csv("testData/retail_db/categories")
.toDF("category_id", "category_department_id", "category_name")

spark.sqlContext.setConf("hive.exec.dynamic.partition", "true")
spark.sqlContext.setConf("hive.exec.dynamic.partition.mode", "nonstrict")

data.write
.partitionBy("category_department_id")
.format("hive")
.mode("overwrite")
.saveAsTable("default.categories_partitioned")

3)
val data = spark.read
.option("inferSchema", "true")
.format("com.databricks.spark.avro")
.option("compression", "snappy")
.load("testData/retail_db/products_avro")

val result = data.rdd.map(x=>(x(0).toString, x.mkString(",")))

result.saveAsSequenceFile("testData/output_retail_db/products_output_avro_1")

4)
val data = sc.sequenceFile("testData/retail_db/products_sequencefile", classOf[org.apache.hadoop.io.Text], classOf[org.apache.hadoop.io.Text])

val result = data.map(x => {val value = x._2.toString.split(",");(value(0),value(1),value(2),value(3),value(4),value(5))}).toDF()

5)
val data = spark.read
.format("com.databricks.spark.avro")
.option("inferSchema", "true")
.load("testData/retail_db/customers-avro")
.toDF("customer_id", "customer_fname", "customer_lname", "customer_email")

val result = data.select($"customer_id", concat(substring($"customer_fname", 0, 1), lit(" "), $"customer_lname").alias("customer_name"))

val output = result.map(x => x.mkString("\t"))

output.write
.option("compression", "bzip2")
.text("testData/output_retail_db/customers_output_avro_1")

//second approach
data.createOrReplaceTempView("customer_view")
val result = spark.sql("""
select customer_id, concat(substring(customer_fname, 0, 1) , ' ', customer_lname) as customer_name from customer_view
""")

val output = result.map(x => x.mkString("\t"))

6)
val data = spark.read
.parquet("testData/retail_db/orders_parquet")

val result = data.filter($"order_status" === "PENDING").select(to_date(from_unixtime($"order_date" / 1000)).alias("orderdate"), $"order_status")

val output = result.filter($"orderdate" >= lit("2013-07-01") && $"orderdate" <= lit("2013-07-31"))

output.write
.option("compression", "snappy")
.json("testData/output_retail_db/orders_parquet_json")

//second approach
data.createOrReplaceTempView("order_view")
val result = spark.sql("""
select to_date(from_unixtime(order_date / 1000)) as order_date, order_status from order_view where order_status = 'PENDING'  
and to_date(from_unixtime(order_date / 1000)) >= '2013-07-01'
and to_date(from_unixtime(order_date / 1000)) <= '2013-07-31'
""").count

7)
val data = spark.sql("""
select * from default.customers where customer_fname like '%Rich%'
""")

data.write
.option("compression", "snappy")
.parquet("testData/output_retail_db/customers_parquet")

8)
val data = spark.read
.option("inferSchema", "true")
.option("delimiter", "\t")
.csv("testData/retail_db/customers-tab-delimited")
.toDF("customer_id", "customer_fname", "customer_lname", "customer_email", "customer_password", "customer_street", "customer_city", "customer_state", "customer_zipcode")

val result = data.filter($"customer_fname".startsWith("M")).groupBy("customer_state").count.show

result.write
.option("compression", "gzip")
.parquet("testData/output_retail_db/customers_avro")

//second approach
data.createOrReplaceTempView("default.customer_view")
val result = spark.sql("""
select customer_state from customer_view where customer_fname like 'M%' group by customer_state
""")

1)
val customer_data = spark.read
.option("inferSchema", "true")
.csv("testData/retail_db/customers")
.toDF("customer_id", "customer_fname", "customer_lname", "customer_email", "customer_password", "customer_street", "customer_city", "customer_state", "customer_zipcode") 

val order_data = spark.read
.option("inferSchema", "true")
.csv("testData/retail_db/orders")
.toDF("order_id", "order_date", "order_customer_id", "order_status") 

val join_result = customer_data.join(order_data, customer_data("customer_id") === order_data("order_customer_id"), "inner")

val output = join_result.groupBy($"customer_id").count.filter($"count" > 5)

val result = output.join(customer_data, output("customer_id") === customer_data("customer_id"), "inner").select($"customer_fname", $"customer_lname", $"count")
.orderBy($"count".desc).filter($"customer_fname".like("M%")).show

result.write
.option("delimiter", "|")
.optiion("compression", "gzip")
.csv("testData/output_retail_db/customer_order")

//second approach
customer_data.createOrReplaceTempView("cust_view")
order_data.createOrReplaceTempView("order_view")
val join_result = spark.sql("""
select cust_view.customer_id, count(cust_view.customer_id) as count
from cust_view, order_view 
where cust_view.customer_id = order_view.order_customer_id
and count(cust_view.customer_id) > 5
and cust_view.customer_fname like 'M%'
group by cust_view.customer_id
order by count(cust_view.customer_id) desc
""")

2)
val product_data = spark.read
.option("inferSchema", "true")
.csv("testData/retail_db/products")
.toDF("product_id", "product_category_id", "product_name", "product_description", "product_price", "product_image")

val category_data = spark.read
.option("inferSchema", "true")
.csv("testData/retail_db/categories")
.toDF("category_id", "dept_id", "category_name")

val result = product_data
.join(category_data, product_data("product_category_id") === category_data("category_id"))
.groupBy($"product_category_id")
.agg(count("product_price").alias("count"), max("product_price").alias("max_price"), min("product_price").alias("min_price"), avg("product_price").alias("avg_price"))
.select($"category_name", $"max_price", $"min_price", $"avg_price", $"count")

//second approach
product_data.createOrReplaceTempView("product_view")
category_data.createOrReplaceTempView("category_view")
val result = spark.sql("""
select category_view.category_id, category_view.category_name, max(product_view.product_price), min(product_view.product_price), avg(product_view.product_price)
from product_view, category_view
where product_view.product_category_id = category_view.category_id
group by category_view.category_id, category_view.category_name
""")

3)
val order_data = spark.read
.option("inferSchema", "true")
.csv("testData/retail_db/orders")
.toDF("order_id", "order_date", "order_customer_id", "order_status") 

order_data.createOrReplaceTempView("order_view")
val result = spark.sql("""
select subString(order_date, 0, 7) as order_date, count(order_date)
from order_view
where order_status = "SUSPECTED_FRAUD"
group by subString(order_date, 0, 7)
order by subString(order_date, 0, 7) desc
""")

4)
val customer_data = spark.read
.option("inferSchema", "true")
.csv("testData/retail_db/customers")
.toDF("customer_id", "customer_fname", "customer_lname", "customer_email", "customer_password", "customer_street", "customer_city", "customer_state", "customer_zipcode") 

val order_data = spark.read
.option("inferSchema", "true")
.csv("testData/retail_db/orders")
.toDF("order_id", "order_date", "order_customer_id", "order_status") 

customer_data.createOrReplaceTempView("customer_view")
order_data.createOrReplaceTempView("order_view")

val result = spark.sql("""
select customer_view.customer_id, customer_view.customer_fname, customer_view.customer_lname, count(1) from customer_view, order_view
where customer_view.customer_id = order_view.order_customer_id
and subString(order_view.order_date, 0, 4) = '2014'
and order_view.order_status = "COMPLETE"
group by customer_view.customer_id, customer_view.customer_fname, customer_view.customer_lname
order by count(1) desc
""")

5)
val customer_data = spark.read
.option("inferSchema", "true")
.csv("testData/retail_db/customers")
.toDF("customer_id", "customer_fname", "customer_lname", "customer_email", "customer_password", "customer_street", "customer_city", "customer_state", "customer_zipcode") 

val order_data = spark.read
.option("inferSchema", "true")
.csv("testData/retail_db/orders")
.toDF("order_id", "order_date", "order_customer_id", "order_status") 

customer_data.createOrReplaceTempView("customer_view")
order_data.createOrReplaceTempView("order_view")

val result = spark.sql("""
select customer_view.customer_id, customer_view.customer_fname, customer_view.customer_lname, count(1) from customer_view, order_view
where customer_view.customer_id = order_view.order_customer_id
and subString(order_view.order_date, 0, 4) = '2013'
and order_view.order_status = "COMPLETE"
group by customer_view.customer_id, customer_view.customer_fname, customer_view.customer_lname
""")


result.write
.format("hive")
.partitionBy("customer_fname")
.saveAsTable("customer_orders")